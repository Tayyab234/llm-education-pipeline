{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-29T10:43:37.125679Z",
     "iopub.status.busy": "2025-12-29T10:43:37.124824Z",
     "iopub.status.idle": "2025-12-29T10:43:40.718736Z",
     "shell.execute_reply": "2025-12-29T10:43:40.718019Z",
     "shell.execute_reply.started": "2025-12-29T10:43:37.125620Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.3)\n",
      "Requirement already satisfied: accelerate in /usr/local/lib/python3.12/dist-packages (1.12.0)\n",
      "Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.12/dist-packages (0.49.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.20.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.36.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2025.11.3)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.5)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.6.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from accelerate) (5.9.5)\n",
      "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from accelerate) (2.8.0+cu126)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.1rc0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (75.2.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (1.13.3)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.5)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (2.27.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (1.11.1.6)\n",
      "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.4.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.6.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2025.11.12)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.0.0->accelerate) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install -U transformers accelerate bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-12-29T10:43:47.982139Z",
     "iopub.status.busy": "2025-12-29T10:43:47.981490Z",
     "iopub.status.idle": "2025-12-29T10:43:47.985827Z",
     "shell.execute_reply": "2025-12-29T10:43:47.985176Z",
     "shell.execute_reply.started": "2025-12-29T10:43:47.982110Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Optional\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-29T10:43:51.324361Z",
     "iopub.status.busy": "2025-12-29T10:43:51.323813Z",
     "iopub.status.idle": "2025-12-29T10:43:51.332006Z",
     "shell.execute_reply": "2025-12-29T10:43:51.331314Z",
     "shell.execute_reply.started": "2025-12-29T10:43:51.324333Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 1. File Readers (txt / pdf / docx)\n",
    "# -----------------------------\n",
    "def read_txt(path: str) -> str:\n",
    "    with open(path, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "        return f.read()\n",
    "\n",
    "def read_docx(path: str) -> str:\n",
    "    from docx import Document\n",
    "    doc = Document(path)\n",
    "    paragraphs = [p.text for p in doc.paragraphs if p.text.strip()]\n",
    "    return \"\\n\\n\".join(paragraphs)\n",
    "\n",
    "def read_pdf_pdfplumber(path: str) -> str:\n",
    "    import pdfplumber\n",
    "    text_pages = []\n",
    "    with pdfplumber.open(path) as pdf:\n",
    "        for page in pdf.pages:\n",
    "            text = page.extract_text() or \"\"\n",
    "            text_pages.append(text)\n",
    "    return \"\\n\\n\".join(text_pages)\n",
    "\n",
    "def read_pdf_pymupdf(path: str) -> str:\n",
    "    import fitz  # PyMuPDF\n",
    "    doc = fitz.open(path)\n",
    "    return \"\\n\\n\".join([page.get_text() for page in doc])\n",
    "    \n",
    "def extract_text_from_path(path: str) -> str:\n",
    "    path = os.path.abspath(path)\n",
    "    ext = os.path.splitext(path)[1].lower()\n",
    "    if ext in ['.txt', '.md']:\n",
    "        return read_txt(path)\n",
    "    elif ext in ['.docx']:\n",
    "        return read_docx(path)\n",
    "    elif ext == '.pdf':\n",
    "        try:\n",
    "            return read_pdf_pdfplumber(path)\n",
    "        except Exception:\n",
    "            return read_pdf_pymupdf(path)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported file type: {ext}\")\n",
    "\n",
    "# -----------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-29T10:43:54.886585Z",
     "iopub.status.busy": "2025-12-29T10:43:54.886063Z",
     "iopub.status.idle": "2025-12-29T10:43:54.891024Z",
     "shell.execute_reply": "2025-12-29T10:43:54.890244Z",
     "shell.execute_reply.started": "2025-12-29T10:43:54.886555Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# 2. Cleaning\n",
    "# -----------------------------\n",
    "def clean_extracted_text(text: str, remove_headfoot=True) -> str:\n",
    "    txt = text.replace('\\r', '\\n')\n",
    "    txt = re.sub(r'\\n\\s+\\n', '\\n\\n', txt)\n",
    "    if remove_headfoot:\n",
    "        txt = re.sub(r'\\nPage\\s*\\d+\\s*\\n', '\\n', txt, flags=re.IGNORECASE)\n",
    "        txt = re.sub(r'^\\s*\\d+\\s*$', '', txt, flags=re.MULTILINE)\n",
    "    txt = re.sub(r'(\\w+)-\\n(\\w+)', r'\\1\\2', txt)\n",
    "    return txt.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-29T10:44:00.526767Z",
     "iopub.status.busy": "2025-12-29T10:44:00.526199Z",
     "iopub.status.idle": "2025-12-29T10:44:00.532608Z",
     "shell.execute_reply": "2025-12-29T10:44:00.531376Z",
     "shell.execute_reply.started": "2025-12-29T10:44:00.526736Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# 3. Tokenizer abstraction\n",
    "# -----------------------------\n",
    "def get_tiktoken_encoder(model_name=\"gpt-4o-mini\"):\n",
    "    try:\n",
    "        import tiktoken\n",
    "        return tiktoken.encoding_for_model(model_name)\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def count_tokens(text: str, encoder=None) -> int:\n",
    "    if encoder is not None:\n",
    "        return len(encoder.encode(text))\n",
    "    return max(1, len(text) // 4)\n",
    "\n",
    "def simple_tokenize_words(text: str) -> List[str]:\n",
    "    return re.findall(r\"\\S+\", text)\n",
    "\n",
    "# -----------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-29T10:44:03.717400Z",
     "iopub.status.busy": "2025-12-29T10:44:03.716727Z",
     "iopub.status.idle": "2025-12-29T10:44:03.723879Z",
     "shell.execute_reply": "2025-12-29T10:44:03.723105Z",
     "shell.execute_reply.started": "2025-12-29T10:44:03.717371Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# 4. Chunking\n",
    "# -----------------------------\n",
    "def chunk_text_by_tokens(\n",
    "    text: str,\n",
    "    max_tokens: int = 900,\n",
    "    overlap_tokens: int = 100,\n",
    "    encoder=None\n",
    "):\n",
    "    assert overlap_tokens < max_tokens, \"overlap_tokens must be < max_tokens\"\n",
    "\n",
    "    tokens = encoder.encode(text)\n",
    "    n = len(tokens)\n",
    "\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    chunk_id = 0\n",
    "\n",
    "    while start < n:\n",
    "        end = min(start + max_tokens, n)\n",
    "\n",
    "        token_slice = tokens[start:end]\n",
    "        content = encoder.decode(token_slice)\n",
    "\n",
    "        chunks.append({\n",
    "            \"id\": f\"chunk_{chunk_id}\",\n",
    "            \"text\": content,\n",
    "            \"token_count\": end - start,\n",
    "            \"token_start\": start,\n",
    "            \"token_end\": end\n",
    "        })\n",
    "\n",
    "        chunk_id += 1\n",
    "\n",
    "        if end == n:\n",
    "            break\n",
    "\n",
    "        start = end - overlap_tokens\n",
    "\n",
    "    return chunks\n",
    "    \n",
    "def process_file_to_chunks(path, max_tokens=900, overlap=100):\n",
    "    txt = extract_text_from_path(path)\n",
    "    txt = clean_extracted_text(txt)\n",
    "\n",
    "    encoder = get_tiktoken_encoder(\"gpt-4o-mini\")\n",
    "\n",
    "    chunks = chunk_text_by_tokens(\n",
    "        txt,\n",
    "        max_tokens=max_tokens,\n",
    "        overlap_tokens=overlap,\n",
    "        encoder=encoder\n",
    "    )\n",
    "\n",
    "    for i, c in enumerate(chunks):\n",
    "        c[\"doc_path\"] = path\n",
    "        c[\"index\"] = i\n",
    "\n",
    "    return chunks\n",
    "\n",
    "\n",
    "# -----------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-29T10:44:08.034033Z",
     "iopub.status.busy": "2025-12-29T10:44:08.033332Z",
     "iopub.status.idle": "2025-12-29T10:44:08.041798Z",
     "shell.execute_reply": "2025-12-29T10:44:08.041088Z",
     "shell.execute_reply.started": "2025-12-29T10:44:08.034002Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from typing import List, Dict\n",
    "\n",
    "def clean_model_output(chunks: List[Dict], math_mode=\"plain\") -> str:\n",
    "    \"\"\"\n",
    "    Generic post-processing of model output chunks.\n",
    "\n",
    "    Parameters:\n",
    "        chunks: list of dicts with 'ideas' or 'text' keys\n",
    "        math_mode: \"plain\" converts LaTeX to human-readable, \"latex\" keeps LaTeX\n",
    "    Returns:\n",
    "        str - combined, cleaned text\n",
    "    \"\"\"\n",
    "    all_texts = []\n",
    "    print(chunks)\n",
    "    for chunk in chunks:\n",
    "        text = chunk.get(\"ideas\") or chunk.get(\"text\") or \"\"\n",
    "\n",
    "        # 1. Unescape common escaped chars\n",
    "        text = text.replace(r'\\\"', '\"')\n",
    "        text = text.replace(r'\\\\n', '\\n')\n",
    "        text = text.replace(r'\\t', '    ')\n",
    "\n",
    "        # 2. Handle LaTeX math\n",
    "        if math_mode == \"plain\":\n",
    "            # remove \\( \\) and $$ $$\n",
    "            text = re.sub(r'\\\\\\(|\\\\\\)', '', text)\n",
    "            text = re.sub(r'\\$\\$', '', text)\n",
    "\n",
    "            # convert common LaTeX symbols to unicode\n",
    "            replacements = {\n",
    "                r'\\\\sigma': 'œÉ',\n",
    "                r'\\\\omega': 'œâ',\n",
    "                r'\\\\theta': 'Œ∏',\n",
    "                r'\\\\pi': 'œÄ',\n",
    "                r'\\\\cos': 'cos',\n",
    "                r'\\\\sin': 'sin',\n",
    "                r'\\\\sqrt': '‚àö',\n",
    "                r'\\\\frac': '/',  # basic fraction replacement\n",
    "            }\n",
    "            for k, v in replacements.items():\n",
    "                text = text.replace(k, v)\n",
    "\n",
    "            # Convert exponents like ^{...} ‚Üí ^(...) for readability\n",
    "            text = re.sub(r'\\^\\{([^}]*)\\}', r'^\\(\\1\\)', text)\n",
    "\n",
    "            # Add spaces around operators if missing\n",
    "            text = re.sub(r'(\\w)([+\\-*/^])', r'\\1 \\2', text)\n",
    "            text = re.sub(r'([+\\-*/^])(\\w)', r'\\1 \\2', text)\n",
    "\n",
    "        # 3. Normalize whitespace\n",
    "        text = re.sub(r'[ \\t]+', ' ', text)        # multiple spaces ‚Üí 1\n",
    "        text = re.sub(r'\\n\\s*\\n', '\\n', text)     # multiple newlines ‚Üí 1\n",
    "        text = text.strip()\n",
    "\n",
    "        all_texts.append(text)\n",
    "\n",
    "    # 4. Combine all chunks with a double newline\n",
    "    return \"\\n\\n\".join(all_texts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-29T10:44:12.040815Z",
     "iopub.status.busy": "2025-12-29T10:44:12.040481Z",
     "iopub.status.idle": "2025-12-29T10:47:22.614367Z",
     "shell.execute_reply": "2025-12-29T10:47:22.612928Z",
     "shell.execute_reply.started": "2025-12-29T10:44:12.040787Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d6d2826b26c4e4a90d29cf0ad488b66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4272f08b9e92462babe2e800d12b1192",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dde16433a34443b195a64badc09e838f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "498e4c0aec444f6fba64ae455a96c440",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7602db0ce4a74bc9b75b39a4562c5c4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/663 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-29 10:44:15.838040: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1767005056.023900      55 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1767005056.090592      55 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1767005056.547049      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1767005056.547078      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1767005056.547081      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1767005056.547084      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b3eac83b06342c09d5e7f2db4eda9b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e2c79b567ef4f04a2b906bc75b51ebf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24a6e5a523d1417ebeacd00c0a5bd24b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00004.safetensors:   0%|          | 0.00/3.86G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50754c7afcbc4049be4f0ca9cc4abaad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00004.safetensors:   0%|          | 0.00/3.95G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02e6e5d239a847e2bee5d3631e09b088",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00004.safetensors:   0%|          | 0.00/3.86G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f3529ea9b9c45af9497f1894494ec51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00004.safetensors:   0%|          | 0.00/3.56G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe8a7d61a635428ca0d7d959a32433b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07350e27cd7a4cda92a84aa34a973d8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/243 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "MODEL_ID = \"Qwen/Qwen2-7B-Instruct\"\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, use_fast=True)\n",
    "\n",
    "model1 = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-29T10:49:51.540777Z",
     "iopub.status.busy": "2025-12-29T10:49:51.540167Z",
     "iopub.status.idle": "2025-12-29T10:49:51.546251Z",
     "shell.execute_reply": "2025-12-29T10:49:51.545418Z",
     "shell.execute_reply.started": "2025-12-29T10:49:51.540750Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "MULTI_TASK_PROMPT_TEMPLATE = \"\"\"You are an assistant that extracts core mathematical or logical ideas.\n",
    "\n",
    "Task:\n",
    "Read the text and extract distinct, atomic ideas.\n",
    "Each idea must represent one clear concept, rule, or definition.\n",
    "\n",
    "Rules:\n",
    "- Output only bullet points.\n",
    "- Each bullet point must start with \"- \".\n",
    "- Preserve symbols and equations exactly as written.\n",
    "- Do not explain, expand, or add examples.\n",
    "- Do not repeat ideas.\n",
    "- Do not add introductions, conclusions, or filler text.\n",
    "\n",
    "Text:\n",
    "\"{text}\"\n",
    "\n",
    "Bullet points:\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "def process_text_local(text: str, max_new_tokens: int = 450):\n",
    "    prompt = MULTI_TASK_PROMPT_TEMPLATE.format(text=text)\n",
    "\n",
    "    inputs = tokenizer(\n",
    "        prompt,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        max_length=1024\n",
    "    )\n",
    "    inputs = {k: v.to(model1.device) for k, v in inputs.items()}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model1.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            temperature=0.3,\n",
    "            top_p=0.9,\n",
    "            do_sample=True,\n",
    "            use_cache=True\n",
    "        )\n",
    "\n",
    "    return tokenizer.decode(\n",
    "        outputs[0][inputs[\"input_ids\"].shape[-1]:],\n",
    "        skip_special_tokens=True\n",
    "    ).strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-29T10:49:56.891792Z",
     "iopub.status.busy": "2025-12-29T10:49:56.891443Z",
     "iopub.status.idle": "2025-12-29T10:49:57.867755Z",
     "shell.execute_reply": "2025-12-29T10:49:57.867183Z",
     "shell.execute_reply.started": "2025-12-29T10:49:56.891767Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "\n",
    "genai.configure(api_key=\"\")\n",
    "\n",
    "model2 = genai.GenerativeModel(\"models/gemini-2.5-flash\")\n",
    "GEMINI_MULTI_TASK_PROMPT = \"\"\"\n",
    "You are an expert educational content generator. Your job is to create high-quality questions \n",
    "from the provided ideas or facts. Each question should match the user's selected task mode \n",
    "and difficulty level.\n",
    "\n",
    "### Modes:\n",
    "You are an intelligent educational content generator.\n",
    "\n",
    "Your task is to generate high-quality educational outputs based strictly on the given ideas.\n",
    "Use controlled creativity while remaining faithful to the topic.\n",
    "\n",
    "---\n",
    "\n",
    "### Supported Modes\n",
    "\n",
    "1. **mcq** ‚Äì Multiple-choice questions (MCQs)\n",
    "   - Generate 1‚Äì2 MCQs per idea (use judgment based on idea complexity).\n",
    "   - Each MCQ must include:\n",
    "       - question\n",
    "       - 4 options (A, B, C, D)\n",
    "       - correct_answer\n",
    "       - brief explanation (1‚Äì2 sentences max)\n",
    "   - Questions should be concise and focused (avoid unnecessary wording).\n",
    "   - Output strictly in JSON:\n",
    "     [\n",
    "       {{\n",
    "         \"question\": \"...\",\n",
    "         \"options\": {{ \"A\": \"...\", \"B\": \"...\", \"C\": \"...\", \"D\": \"...\" }},\n",
    "         \"correct_answer\": \"A\",\n",
    "         \"explanation\": \"...\"\n",
    "       }}\n",
    "     ]\n",
    "\n",
    "2. **qa** ‚Äì Question‚ÄìAnswer pairs\n",
    "   - Generate 1‚Äì2 questions per idea if meaningful.\n",
    "   - Questions must be clear and specific.\n",
    "   - Answers must be direct, factual, and concise.\n",
    "   - Keep question length proportional to answer length.\n",
    "   - Do NOT include explanations.\n",
    "   - Output JSON:\n",
    "     [\n",
    "       {{\n",
    "         \"question\": \"...\",\n",
    "         \"answer\": \"...\"\n",
    "       }}\n",
    "     ]\n",
    "\n",
    "3. **true_false** ‚Äì True/False statements\n",
    "   - Generate at least one statement per idea.\n",
    "   - Randomly vary between True and False.\n",
    "   - Statements must be short, precise, and unambiguous (avoid long compound sentences).\n",
    "   - Output JSON:\n",
    "     [\n",
    "       {{\n",
    "         \"statement\": \"...\",\n",
    "         \"answer\": \"True\"\n",
    "       }}\n",
    "     ]\n",
    "\n",
    "4. **fill_blank** ‚Äì Fill-in-the-blank\n",
    "   - Generate 1‚Äì2 questions per idea.\n",
    "   - Hide a key term, symbol, formula component, or concept.\n",
    "   - The blank should target an essential element, not trivial words.\n",
    "   - Output JSON:\n",
    "     [\n",
    "       {{\n",
    "         \"question\": \"... ____ ...\",\n",
    "         \"answer\": \"...\"\n",
    "       }}\n",
    "     ]\n",
    "\n",
    "5. **summary** ‚Äì Conceptual summary\n",
    "   - Produce a concise academic summary of the given ideas.\n",
    "   - Length: a short paragraph (max 120 words).\n",
    "   - No examples, no extra explanations.\n",
    "   - Output JSON:\n",
    "     {{\n",
    "       \"summary\": \"...\"\n",
    "     }}\n",
    "\n",
    "---\n",
    "\n",
    "### Difficulty Level (Bloom‚Äôs Taxonomy)\n",
    "\n",
    "- **Easy:** Recall, definitions, direct facts, simple relationships.\n",
    "- **Medium:** Application, comparison, moderate reasoning.\n",
    "- **Hard:** Multi-step reasoning, synthesis of ideas, deeper conceptual understanding.\n",
    "\n",
    "---\n",
    "\n",
    "### Global Rules\n",
    "\n",
    "- Align all outputs strictly with the given **mode** and **difficulty level**.\n",
    "- Preserve all mathematical notation, symbols, and formulas **exactly as given**.\n",
    "- Maintain balance:\n",
    "  - Do not generate overly long questions with very short answers.\n",
    "  - Prefer clarity over verbosity.\n",
    "- Use creativity only to improve question quality, not to add new concepts.\n",
    "- Output **valid JSON only** ‚Äî no extra text before or after.\n",
    "\n",
    "---\n",
    "\n",
    "Mode: {mode}\n",
    "Difficulty: {difficulty_level}\n",
    "\n",
    "Ideas:\n",
    "{ideas}\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def generate_questions_with_gemini(ideas: str,mode:str ,difficulty:str, max_output_tokens=5000):\n",
    "    prompt = GEMINI_MULTI_TASK_PROMPT.format(ideas=ideas,difficulty_level=difficulty,mode=mode)\n",
    "    response = model2.generate_content(\n",
    "        prompt,\n",
    "        generation_config=genai.GenerationConfig(\n",
    "            max_output_tokens=max_output_tokens,\n",
    "            temperature=0.8,   # creativity\n",
    "            top_p=0.9\n",
    "        )\n",
    "    )\n",
    "    return response.text.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-29T10:50:07.355768Z",
     "iopub.status.busy": "2025-12-29T10:50:07.355031Z",
     "iopub.status.idle": "2025-12-29T10:50:07.361225Z",
     "shell.execute_reply": "2025-12-29T10:50:07.360454Z",
     "shell.execute_reply.started": "2025-12-29T10:50:07.355735Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def chunk_text_by_rules(\n",
    "    text: str,\n",
    "    max_chars=4000,\n",
    "    max_lines=25\n",
    "):\n",
    "    \"\"\"\n",
    "    Splits text into safe chunks using character and line limits.\n",
    "    \"\"\"\n",
    "    lines = [line.strip() for line in text.split(\"\\n\") if line.strip()]\n",
    "    chunks = []\n",
    "\n",
    "    current_chunk = []\n",
    "    current_chars = 0\n",
    "\n",
    "    for line in lines:\n",
    "        line_len = len(line)\n",
    "\n",
    "        if (\n",
    "            len(current_chunk) >= max_lines\n",
    "            or current_chars + line_len > max_chars\n",
    "        ):\n",
    "            chunks.append(\"\\n\".join(current_chunk))\n",
    "            current_chunk = []\n",
    "            current_chars = 0\n",
    "\n",
    "        current_chunk.append(line)\n",
    "        current_chars += line_len\n",
    "\n",
    "    if current_chunk:\n",
    "        chunks.append(\"\\n\".join(current_chunk))\n",
    "\n",
    "    return chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-29T11:06:32.860756Z",
     "iopub.status.busy": "2025-12-29T11:06:32.860410Z",
     "iopub.status.idle": "2025-12-29T11:06:32.867228Z",
     "shell.execute_reply": "2025-12-29T11:06:32.866512Z",
     "shell.execute_reply.started": "2025-12-29T11:06:32.860729Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "\n",
    "def clean_gemini_output(raw_text: str) -> str:\n",
    "    \"\"\"\n",
    "    Minimal cleanup:\n",
    "    - Remove markdown JSON fences\n",
    "    - Remove unnecessary escape slashes\n",
    "    - Drop incomplete JSON objects\n",
    "    \"\"\"\n",
    "\n",
    "    # 0. Remove Markdown code fences like ```json ... ```\n",
    "    text = raw_text.strip()\n",
    "    text = re.sub(r\"^```(?:json)?\\s*\", \"\", text)\n",
    "    text = re.sub(r\"\\s*```$\", \"\", text)\n",
    "\n",
    "    # 1. Remove escaped newlines and quotes (if any)\n",
    "    text = text.replace(\"\\\\n\", \"\\n\")\n",
    "    text = text.replace('\\\\\"', '\"')\n",
    "\n",
    "    # 2. Try loading JSON safely\n",
    "    try:\n",
    "        data = json.loads(text)\n",
    "    except json.JSONDecodeError:\n",
    "        # If JSON is broken, return original (do NOT guess)\n",
    "        return raw_text.strip()\n",
    "\n",
    "    # 3. Remove incomplete objects\n",
    "    if isinstance(data, list):\n",
    "        clean_data = []\n",
    "        for item in data:\n",
    "            if isinstance(item, dict):\n",
    "                s = json.dumps(item).strip()\n",
    "                if s.startswith(\"{\") and s.endswith(\"}\"):\n",
    "                    clean_data.append(item)\n",
    "        data = clean_data\n",
    "\n",
    "    # 4. Dump clean JSON\n",
    "    return json.dumps(data, indent=2, ensure_ascii=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-29T10:50:19.033494Z",
     "iopub.status.busy": "2025-12-29T10:50:19.032805Z",
     "iopub.status.idle": "2025-12-29T10:50:19.040139Z",
     "shell.execute_reply": "2025-12-29T10:50:19.039469Z",
     "shell.execute_reply.started": "2025-12-29T10:50:19.033463Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def process_ideas_to_questions(\n",
    "    ideas_txt: str,\n",
    "    mode: str,\n",
    "    difficulty_level: str,\n",
    "    output_json: str = None\n",
    "):\n",
    "    # Default output file name\n",
    "    if output_json is None:\n",
    "        output_json = ideas_txt.replace(\n",
    "            \".txt\", f\"_{mode}_{difficulty_level}.json\"\n",
    "        )\n",
    "\n",
    "    # Read full ideas text\n",
    "    with open(ideas_txt, \"r\", encoding=\"utf-8\") as f:\n",
    "        full_text = f.read()\n",
    "\n",
    "    # Chunk text to control tokens\n",
    "    chunks = chunk_text_by_rules(full_text)\n",
    "    print(f\"üîπ Total chunks: {len(chunks)}\")\n",
    "\n",
    "    all_results = []  # ‚Üê combined output\n",
    "\n",
    "    for idx, chunk in enumerate(chunks, start=1):\n",
    "        print(f\"‚û°Ô∏è Processing chunk {idx}/{len(chunks)}\")\n",
    "\n",
    "        raw_response = generate_questions_with_gemini(\n",
    "            ideas=chunk,\n",
    "            mode=mode,\n",
    "            difficulty=difficulty_level\n",
    "        )\n",
    "\n",
    "        # Minimal cleaning\n",
    "        clean_response = clean_gemini_output(raw_response)\n",
    "\n",
    "        # Parse JSON safely\n",
    "        try:\n",
    "            parsed = json.loads(clean_response)\n",
    "        except json.JSONDecodeError:\n",
    "            print(f\"‚ö†Ô∏è Skipping chunk {idx}: invalid JSON\")\n",
    "            continue\n",
    "\n",
    "        # Combine results\n",
    "        if isinstance(parsed, list):\n",
    "            all_results.extend(parsed)\n",
    "        else:\n",
    "            all_results.append(parsed)\n",
    "\n",
    "    # Save combined output\n",
    "    with open(output_json, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(all_results, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "    print(f\"‚úÖ Done! {len(all_results)} items saved to {output_json}\")\n",
    "    \n",
    "    return output_json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-29T10:50:22.832083Z",
     "iopub.status.busy": "2025-12-29T10:50:22.831339Z",
     "iopub.status.idle": "2025-12-29T10:50:22.837058Z",
     "shell.execute_reply": "2025-12-29T10:50:22.836356Z",
     "shell.execute_reply.started": "2025-12-29T10:50:22.832052Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def process_transcript_to_ideas(\n",
    "    path: str,\n",
    "    out_txt: str = \"ideas.txt\"\n",
    "):\n",
    "    chunks = process_file_to_chunks(path, max_tokens=900, overlap=100)\n",
    "    results = []\n",
    "\n",
    "    for chunk in chunks:\n",
    "        print(f\"Processing {chunk['id']} ({chunk['token_count']} tokens)...\")\n",
    "        ideas_or_summary = process_text_local(chunk[\"text\"])\n",
    "        results.append({\n",
    "            \"id\": chunk[\"id\"],\n",
    "            \"ideas\": ideas_or_summary\n",
    "        })\n",
    "\n",
    "    clean_text = clean_model_output(results, math_mode=\"plain\")\n",
    "\n",
    "    with open(out_txt, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(clean_text)\n",
    "\n",
    "    print(f\"‚úÖ Ideas saved to {out_txt}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-29T11:13:12.507274Z",
     "iopub.status.busy": "2025-12-29T11:13:12.506941Z",
     "iopub.status.idle": "2025-12-29T11:13:14.079091Z",
     "shell.execute_reply": "2025-12-29T11:13:14.078500Z",
     "shell.execute_reply.started": "2025-12-29T11:13:12.507248Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7862\n",
      "It looks like you are running Gradio on a hosted Jupyter notebook, which requires `share=True`. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
      "\n",
      "* Running on public URL: https://57486b9ba23113dda2.gradio.live\n",
      "\n",
      "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://57486b9ba23113dda2.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:    Exception in ASGI application\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/uvicorn/protocols/http/h11_impl.py\", line 403, in run_asgi\n",
      "    result = await app(  # type: ignore[func-returns-value]\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/uvicorn/middleware/proxy_headers.py\", line 60, in __call__\n",
      "    return await self.app(scope, receive, send)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/fastapi/applications.py\", line 1133, in __call__\n",
      "    await super().__call__(scope, receive, send)\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/starlette/applications.py\", line 113, in __call__\n",
      "    await self.middleware_stack(scope, receive, send)\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/starlette/middleware/errors.py\", line 186, in __call__\n",
      "    raise exc\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/starlette/middleware/errors.py\", line 164, in __call__\n",
      "    await self.app(scope, receive, _send)\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/gradio/brotli_middleware.py\", line 74, in __call__\n",
      "    return await self.app(scope, receive, send)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/gradio/route_utils.py\", line 882, in __call__\n",
      "    await self.app(scope, receive, send)\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/starlette/middleware/exceptions.py\", line 63, in __call__\n",
      "    await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/starlette/_exception_handler.py\", line 53, in wrapped_app\n",
      "    raise exc\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/starlette/_exception_handler.py\", line 42, in wrapped_app\n",
      "    await app(scope, receive, sender)\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/fastapi/middleware/asyncexitstack.py\", line 18, in __call__\n",
      "    await self.app(scope, receive, send)\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/starlette/routing.py\", line 716, in __call__\n",
      "    await self.middleware_stack(scope, receive, send)\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/starlette/routing.py\", line 736, in app\n",
      "    await route.handle(scope, receive, send)\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/starlette/routing.py\", line 290, in handle\n",
      "    await self.app(scope, receive, send)\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/fastapi/routing.py\", line 123, in app\n",
      "    await wrap_app_handling_exceptions(app, request)(scope, receive, send)\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/starlette/_exception_handler.py\", line 53, in wrapped_app\n",
      "    raise exc\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/starlette/_exception_handler.py\", line 42, in wrapped_app\n",
      "    await app(scope, receive, sender)\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/fastapi/routing.py\", line 109, in app\n",
      "    response = await f(request)\n",
      "               ^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/fastapi/routing.py\", line 389, in app\n",
      "    raw_response = await run_endpoint_function(\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/fastapi/routing.py\", line 288, in run_endpoint_function\n",
      "    return await dependant.call(**values)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/gradio/routes.py\", line 1671, in get_upload_progress\n",
      "    await asyncio.wait_for(\n",
      "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 520, in wait_for\n",
      "    return await fut\n",
      "           ^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/gradio/route_utils.py\", line 528, in is_tracked\n",
      "    return await self._signals[upload_id].wait()\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3.12/asyncio/locks.py\", line 209, in wait\n",
      "    fut = self._get_loop().create_future()\n",
      "          ^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3.12/asyncio/mixins.py\", line 20, in _get_loop\n",
      "    raise RuntimeError(f'{self!r} is bound to a different event loop')\n",
      "RuntimeError: <asyncio.locks.Event object at 0x7b334eb66d20 [unset]> is bound to a different event loop\n"
     ]
    }
   ],
   "source": [
    "#GUI\n",
    "import gradio as gr\n",
    "\n",
    "def run_pipeline(text_input, file_obj, mode, difficulty):\n",
    "    # -----------------------------\n",
    "    # Step 1: Load text\n",
    "    # -----------------------------\n",
    "    if file_obj:\n",
    "        text = Path(file_obj.name).read_text(encoding=\"utf-8\")\n",
    "    else:\n",
    "        text = text_input\n",
    "\n",
    "    if not text.strip():\n",
    "        return \"‚ö†Ô∏è No input text provided.\"\n",
    "\n",
    "    # Save raw input\n",
    "    with open(\"/kaggle/working/input.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(text)\n",
    "\n",
    "    # -----------------------------\n",
    "    # Step 2: Stage-1 ‚Üí Ideas / Summary\n",
    "    # -----------------------------\n",
    "    process_transcript_to_ideas(\n",
    "        path=\"/kaggle/working/input.txt\",\n",
    "        out_txt=\"/kaggle/working/ideas.txt\"\n",
    "    )\n",
    "\n",
    "    if not Path(\"ideas.txt\").exists():\n",
    "        return \"‚ö†Ô∏è Failed to generate ideas.\"\n",
    "\n",
    "    # -----------------------------\n",
    "    # Step 3: Stage-2 ‚Üí Questions\n",
    "    # -----------------------------\n",
    "    output_file=process_ideas_to_questions(\n",
    "        ideas_txt=\"/kaggle/working/ideas.txt\",\n",
    "        mode=mode,\n",
    "        difficulty_level=difficulty\n",
    "    )\n",
    "\n",
    "    # -----------------------------\n",
    "    # Step 4: Load output\n",
    "    # -----------------------------\n",
    "\n",
    "    if not Path(output_file).exists():\n",
    "        return \"‚ö†Ô∏è No output generated.\"\n",
    "\n",
    "    with open(output_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    return json.dumps(data, indent=2, ensure_ascii=False)\n",
    "\n",
    "iface = gr.Interface(\n",
    "    fn=run_pipeline,\n",
    "    inputs=[\n",
    "        gr.Textbox(lines=10, label=\"Input Text\"),\n",
    "        gr.File(file_types=[\".txt\"], label=\"Upload Text File\"),\n",
    "        gr.Dropdown([\"mcq\",\"qa\",\"true_false\",\"fill_blank\",\"summary\"], label=\"Mode\"),\n",
    "        gr.Dropdown([\"easy\",\"medium\",\"hard\"], label=\"Difficulty\")\n",
    "    ],\n",
    "    outputs=gr.Textbox(lines=20, label=\"Output (JSON)\"),\n",
    "    title=\"AI Question Generator\",\n",
    "    description=\"Two-stage LLM pipeline: Ideas ‚Üí Questions\"\n",
    ")\n",
    "\n",
    "iface.launch()\n",
    " \n",
    "           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#CLI\n",
    "# 7. Pipeline Runner\n",
    "# -----------------------------\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     # üß† Step 0 ‚Üí User Input\n",
    "#     mode = input(\"Enter mode (summary / ideas / qa / mcq / true_false / fill_blank): \").strip().lower()\n",
    "\n",
    "\n",
    "#     # File to process\n",
    "#     input_path = r\"/kaggle/input/documents/transcript_2.txt\"\n",
    "    \n",
    "#     if mode in [\"mcq\", \"true_false\", \"fill_blank\",\"qa\",\"summary\"]:\n",
    "#           # Optional: difficulty for modes that need it\n",
    "#         difficulty_level = input(\"Enter difficulty (easy / medium / hard): \").strip().lower()\n",
    "#         #process_transcript_to_ideas(input_path)\n",
    "#         process_ideas_to_questions(\"/kaggle/working/ideas.txt\", difficulty_level=difficulty_level, mode=mode)\n",
    "\n",
    "#     elif mode == \"ideas\":\n",
    "#         # QA-prep may just save Phi2 results or call a different function\n",
    "#         process_transcript_to_ideas(input_path)\n",
    "#         print(\"‚úÖ ideas has been generated and  saved in ideas.txt.\")\n",
    "\n",
    "#     else:\n",
    "#         print(f\"‚ö†Ô∏è Unknown mode: {mode}\")\n",
    "    \n",
    "                                                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-29T10:35:12.619430Z",
     "iopub.status.busy": "2025-12-29T10:35:12.619164Z",
     "iopub.status.idle": "2025-12-29T10:35:12.831546Z",
     "shell.execute_reply": "2025-12-29T10:35:12.830761Z",
     "shell.execute_reply.started": "2025-12-29T10:35:12.619394Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models/embedding-gecko-001 ‚Üí ['embedText', 'countTextTokens']\n",
      "models/gemini-2.5-flash ‚Üí ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent']\n",
      "models/gemini-2.5-pro ‚Üí ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent']\n",
      "models/gemini-2.0-flash-exp ‚Üí ['generateContent', 'countTokens', 'bidiGenerateContent']\n",
      "models/gemini-2.0-flash ‚Üí ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent']\n",
      "models/gemini-2.0-flash-001 ‚Üí ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent']\n",
      "models/gemini-2.0-flash-exp-image-generation ‚Üí ['generateContent', 'countTokens', 'bidiGenerateContent']\n",
      "models/gemini-2.0-flash-lite-001 ‚Üí ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent']\n",
      "models/gemini-2.0-flash-lite ‚Üí ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent']\n",
      "models/gemini-2.0-flash-lite-preview-02-05 ‚Üí ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent']\n",
      "models/gemini-2.0-flash-lite-preview ‚Üí ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent']\n",
      "models/gemini-exp-1206 ‚Üí ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent']\n",
      "models/gemini-2.5-flash-preview-tts ‚Üí ['countTokens', 'generateContent']\n",
      "models/gemini-2.5-pro-preview-tts ‚Üí ['countTokens', 'generateContent', 'batchGenerateContent']\n",
      "models/gemma-3-1b-it ‚Üí ['generateContent', 'countTokens']\n",
      "models/gemma-3-4b-it ‚Üí ['generateContent', 'countTokens']\n",
      "models/gemma-3-12b-it ‚Üí ['generateContent', 'countTokens']\n",
      "models/gemma-3-27b-it ‚Üí ['generateContent', 'countTokens']\n",
      "models/gemma-3n-e4b-it ‚Üí ['generateContent', 'countTokens']\n",
      "models/gemma-3n-e2b-it ‚Üí ['generateContent', 'countTokens']\n",
      "models/gemini-flash-latest ‚Üí ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent']\n",
      "models/gemini-flash-lite-latest ‚Üí ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent']\n",
      "models/gemini-pro-latest ‚Üí ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent']\n",
      "models/gemini-2.5-flash-lite ‚Üí ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent']\n",
      "models/gemini-2.5-flash-image-preview ‚Üí ['generateContent', 'countTokens', 'batchGenerateContent']\n",
      "models/gemini-2.5-flash-image ‚Üí ['generateContent', 'countTokens', 'batchGenerateContent']\n",
      "models/gemini-2.5-flash-preview-09-2025 ‚Üí ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent']\n",
      "models/gemini-2.5-flash-lite-preview-09-2025 ‚Üí ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent']\n",
      "models/gemini-3-pro-preview ‚Üí ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent']\n",
      "models/gemini-3-flash-preview ‚Üí ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent']\n",
      "models/gemini-3-pro-image-preview ‚Üí ['generateContent', 'countTokens', 'batchGenerateContent']\n",
      "models/nano-banana-pro-preview ‚Üí ['generateContent', 'countTokens', 'batchGenerateContent']\n",
      "models/gemini-robotics-er-1.5-preview ‚Üí ['generateContent', 'countTokens']\n",
      "models/gemini-2.5-computer-use-preview-10-2025 ‚Üí ['generateContent', 'countTokens']\n",
      "models/deep-research-pro-preview-12-2025 ‚Üí ['generateContent', 'countTokens']\n",
      "models/embedding-001 ‚Üí ['embedContent']\n",
      "models/text-embedding-004 ‚Üí ['embedContent']\n",
      "models/gemini-embedding-exp-03-07 ‚Üí ['embedContent', 'countTextTokens', 'countTokens']\n",
      "models/gemini-embedding-exp ‚Üí ['embedContent', 'countTextTokens', 'countTokens']\n",
      "models/gemini-embedding-001 ‚Üí ['embedContent', 'countTextTokens', 'countTokens', 'asyncBatchEmbedContent']\n",
      "models/aqa ‚Üí ['generateAnswer']\n",
      "models/imagen-4.0-generate-preview-06-06 ‚Üí ['predict']\n",
      "models/imagen-4.0-ultra-generate-preview-06-06 ‚Üí ['predict']\n",
      "models/imagen-4.0-generate-001 ‚Üí ['predict']\n",
      "models/imagen-4.0-ultra-generate-001 ‚Üí ['predict']\n",
      "models/imagen-4.0-fast-generate-001 ‚Üí ['predict']\n",
      "models/veo-2.0-generate-001 ‚Üí ['predictLongRunning']\n",
      "models/veo-3.0-generate-001 ‚Üí ['predictLongRunning']\n",
      "models/veo-3.0-fast-generate-001 ‚Üí ['predictLongRunning']\n",
      "models/veo-3.1-generate-preview ‚Üí ['predictLongRunning']\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "API_KEY = \"\"\n",
    "url = \"https://generativelanguage.googleapis.com/v1beta/models\"\n",
    "\n",
    "headers = {\n",
    "    \"Content-Type\": \"application/json\",\n",
    "    \"x-goog-api-key\": API_KEY\n",
    "}\n",
    "\n",
    "response = requests.get(url, headers=headers)\n",
    "data = response.json()\n",
    "\n",
    "for model in data.get(\"models\", []):\n",
    "    print(\n",
    "        model[\"name\"],\n",
    "        \"‚Üí\",\n",
    "        model.get(\"supportedGenerationMethods\", [])\n",
    "    )\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 9106450,
     "sourceId": 14269873,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31236,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
